

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic
        Segmentation Using Diffusion Models</title>
</head>

<body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
	<br>
	<center>
	<span style="font-size:36px">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models</span><br><br><br>
	</center>
	<table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="100px">
              <center>
                <span style="font-size:16px">Weijia Wu</a><sup>1,3</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Yuzhong Zhao<sup>2</sup></span>
                </center>
              </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Mike Zheng Shou</a><sup>3 *</sup></span>
                </center>
              </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Hong Zhou</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Chunhua Shen</a><sup>1</sup></span>
                </center>
         
		        
          </td></tr>
        </tbody></table><br>
	
	  <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Zhejiang University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>University of Chinese Academy of Sciences</span>
                </center>
                </td>
                    </td>
                        <td align="center" width="300px">
                  <center>
                        <span style="font-size:16px"><sup>3</sup>National University of Singapore</span>
                    </center>
                    </td>
        </tr></tbody></table>
	
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/weijiawu/DiffuMask"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="noe"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./resources/noen"> [BibTeX]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
	
      <br><hr>
      <br>         
      
      <center>
          <img src="./resources/fig1.png" alt="alt text" style="width: 100%; object-fit: cover; max-width:100%;"></a>
        </center>
        <p style="text-align:justify; text-justify:inter-ideograph;"><left>
            DiffuMask synthesizes realistic images and mask annotations by exploiting the attention maps of the diffusion model. Without human effort and localization (i.e., box and mask) annotation, DiffuMask is capable of producing high-quality semantic.
      </left></p>
        <br> 

      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      </p><div class="container">
        <div class="text" width="400px"> 
          <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
                Collecting and annotating images with pixel-wise labels
                is time-consuming and laborious. In contrast, synthetic
                data can be freely available using a generative model (e.g.,
                DALL-E, Stable Diffusion). In this paper, we show that
                it is possible to automatically obtain accurate semantic
                masks of synthetic images generated by the pre-trained
                Stable Diffusion, which uses only text-image pairs during
                training. Our approach, called DiffuMask, exploits
                the potential of the cross-attention map between text and
                image, which is natural and seamless to extend the textdriven
                image synthesis to semantic mask generation. DiffuMask
                uses text-guided cross-attention information to localize
                class/word-specific regions, which are combined with
                practical techniques to create a novel high-resolution and
                class-discriminative pixel-wise mask. The methods help to
                reduce data collection and annotation costs obviously. Experiments
                demonstrate that the existing segmentation methods
                trained on synthetic data of DiffuMask can achieve a
                competitive performance over the counterpart of real data
                (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask
                presents promising performance, close to the stateof-
                the-art result of real data (within 3% mIoU gap). Moreover,
                in the open-vocabulary segmentation (zero-shot) setting,
                DiffuMask achieves a new SOTA result on Unseen
                class of VOC 2012.
          </left></p>
        </div>
      </div>
      <br>
      
      <center><h2>Can the attention map be used as mask annotation?</h2></center>
      <div class="container">
        <div class="image" width="350px">
          <center><p><img class="center" src="./resources/1679384709715.jpg" width="500px"></p></center>
        </div>
        <div class="text" width="250px">
          <p> A ‘good’ mask annotation satisfy two conditions:
          1) class-discriminative. 2) high-resolution, precise mask.
          
          The average map shows the possibility for us to use for semantic segmentation,
          where it is class-discriminative and fine-grained.
          </p>
        </div>
      </div>
      
      <p><img class="center" src="./resources/fig7.png" width="600px"></p>
          
          
      <hr>
      
      <br>
      <center> <h2> Pipeline </h2> </center>
      <p><img class="left" src="./resources/fig2.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
          Pipeline for DiffuMask with a given prompt: ‘Photo of a [sub-class] car in the street’. DiffuMask mainly
          includes three steps: 1) Prompt engineering is used to enhance the diversity and reality of prompt language. 2) Image and
          mask generation and refinement with adaptive threshold from AffinityNet. 3) Noise learning is designed to further improve
          the quality of data via filtering the noisy label.
         </left></p>
      
      <br>
      <hr>
      <center><h2>Protocol-I: Semantic Segmentation</h2></center>
      <p><b>Quantitative result for Protocol-I evaluation on Semantic Segmentation </b> </p>

      <p><img class="center" src="./resources/1679384836897.jpg" width="800px"></p>

      <p><b>Qualitative Results</b> </p>
      <p><left>
       todo
      </left></p>
      <p><img class="center" src="./resources/1679384940012.jpg" width="800px"></p>
      <br>
      <hr>

      <center><h2>Protocol-II: Open-vocabulary Segmentation</h2></center>
      <p><b>Comparison with the previous ZS3 methods on PASCAL VOC. </b> </p>
      <div class="container">
        <div class="image" width="200px">
          <center><p><img class="center" src="./resources/1679388249838.jpg" width="500px"></p></center>
        </div>
        <div class="text" width="250px"> 
          <p> The “Seen”, “Unseen”, and “Harmonic” denote mIoU of seen categories, unseen categories, and their harmonic mean. These ZS3 methods are trained on PASCAL-VOC training set.
          </p>
        </div>
      </div>
      
      <center><h2>Protocol-III: Domain Generalization</h2></center>
      <p><b>Performance for Domain Generalization between
          different datasets. </b> </p>
      <div class="container">
        <div class="image" width="400px">
          <center><p><img class="center" src="./resources/fig4.png" width="500px"></p></center>
        </div>
        <div class="text" width="150px">
          <p> The Table presents the results for cross-dataset validation,
          which can evaluate the generalization of data. Compared
          with real data, DiffuMask show powerful effectiveness on
          domain generalization, e.g., 69.5% with DiffuMask v.s
          68.0 with ADE20K on VOC 2012 val.
          </p>
        </div>
      </div>

      <center><h2>Ablation Study</h2></center>
      <div class="container">
        <div class="image" width="850px">
          <center><p><img class="center" src="./resources/1679385037897.jpg" width="800px"></p></center>
        </div>
        </div>
      </div>

      <br>
      <hr>
      <center> <h2> Acknowledgements </h2> </center>
      <p> 
	      Based on a template by <a href="https://lipurple.github.io/">
              Ziyi Li</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>
<br>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
